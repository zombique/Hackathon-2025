#!/bin/bash
set -e

echo "üöÄ Starting FinCrime Pipeline end-to-end on Vertex AI"

# --- CONFIG ---
PROJECT_ID=$(gcloud config get-value project 2>/dev/null || echo "")
REGION=$(gcloud config get-value ai/region 2>/dev/null || echo "")
if [[ -z "$REGION" ]]; then
  REGION=$(gcloud config get-value compute/region 2>/dev/null || echo "")
fi
if [[ -z "$REGION" ]]; then
  REGION="asia-southeast1"   # ‚úÖ fallback
fi
JOB_NAME="fincrime-pipeline-job-$(date +%s)"

STAGING_BUCKET="gs://${PROJECT_ID}-fincrime-pipeline-root"
OUTPUT_BUCKET="gs://${PROJECT_ID}-fincrime-outputs"

echo "‚úÖ INFO: Using Project: $PROJECT_ID"
echo "‚úÖ INFO: Using Region: $REGION"
echo "‚úÖ INFO: Job Name: $JOB_NAME"

# --- BUCKETS ---
if gsutil ls -b "$STAGING_BUCKET" >/dev/null 2>&1; then
  echo "‚ÑπÔ∏è Reusing existing staging bucket: $STAGING_BUCKET"
else
  echo "‚úÖ INFO: Creating staging bucket: $STAGING_BUCKET"
  gsutil mb -l "$REGION" "$STAGING_BUCKET"
fi

if gsutil ls -b "$OUTPUT_BUCKET" >/dev/null 2>&1; then
  echo "‚ÑπÔ∏è Reusing existing output bucket: $OUTPUT_BUCKET"
else
  echo "‚úÖ INFO: Creating output bucket: $OUTPUT_BUCKET"
  gsutil mb -l "$REGION" "$OUTPUT_BUCKET"
fi

# --- INPUT FILE ---
if gsutil ls "$OUTPUT_BUCKET/transactions_sample.csv" >/dev/null 2>&1; then
  echo "‚ÑπÔ∏è Reusing existing transactions_sample.csv in $OUTPUT_BUCKET"
else
  echo "‚úÖ INFO: Uploading transactions_sample.csv"
  gsutil cp transactions_sample.csv "$OUTPUT_BUCKET/"
fi

# --- UPLOAD CODE FILES ---
echo "‚úÖ INFO: Uploading pipeline code to $STAGING_BUCKET/code/"
gsutil cp run_pipeline_auto.py "$STAGING_BUCKET/code/"
gsutil cp fincrime_pipeline.py "$STAGING_BUCKET/code/" || true
gsutil cp fincrime_pipeline.yaml "$STAGING_BUCKET/code/" || true
gsutil cp requirements.txt "$STAGING_BUCKET/code/" || true

# --- SUBMIT CUSTOM JOB ---
echo "‚úÖ INFO: Submitting Custom Job to Vertex AI..."

JOB_ID=$(gcloud ai custom-jobs create \
  --project="$PROJECT_ID" \
  --region="$REGION" \
  --display-name="$JOB_NAME" \
  --format="value(name)" \
  --worker-pool-spec=machine-type=n1-standard-4,executor-image-uri=us-docker.pkg.dev/vertex-ai/training/python:3.10,script=run_pipeline_auto.py,requirements=requirements.txt \
  --args="--project=$PROJECT_ID","--region=$REGION","--staging-bucket=$STAGING_BUCKET","--gcs-input-uri=$OUTPUT_BUCKET/transactions_sample.csv","--export-uri=$OUTPUT_BUCKET/fincrime_output/","--model=gemini-1.5-flash")

echo "‚úÖ INFO: Custom Job submitted: $JOB_ID"
echo "üì° Streaming logs... (Ctrl+C to stop)"

# --- STREAM LOGS ---
gcloud ai custom-jobs stream-logs "$JOB_ID" --project="$PROJECT_ID" --region="$REGION"
