#!/bin/bash
set -euo pipefail

echo "ðŸš€ Starting FinCrime Pipeline end-to-end on Vertex AI"
LOG_FILE="run_pipeline.log"
exec > >(tee -a "$LOG_FILE") 2>&1

# Detect project and region
PROJECT=$(gcloud config get-value project)
REGION=$(gcloud config get-value compute/region)
if [[ -z "$REGION" || "$REGION" == "(unset)" ]]; then
  REGION="asia-southeast1"
fi

SERVICE_ACCOUNT="vertex-ai-sa@${PROJECT}.iam.gserviceaccount.com"

echo "âœ… INFO: Using Project: $PROJECT"
echo "âœ… INFO: Using Region: $REGION"
echo "âœ… INFO: Using Service Account: $SERVICE_ACCOUNT"

# Buckets (unique suffix to avoid conflicts)
SUFFIX=$(date +%s)
STAGING_BUCKET="gs://${PROJECT}-fincrime-pipeline-root-${SUFFIX}"
OUTPUT_BUCKET="gs://${PROJECT}-fincrime-outputs-${SUFFIX}"

# Create buckets if not exist
echo "âœ… INFO: Creating buckets..."
gsutil mb -l "$REGION" -p "$PROJECT" "$STAGING_BUCKET" || echo "â„¹ï¸ Bucket $STAGING_BUCKET already exists"
gsutil mb -l "$REGION" -p "$PROJECT" "$OUTPUT_BUCKET" || echo "â„¹ï¸ Bucket $OUTPUT_BUCKET already exists"

# Upload input data
echo "âœ… INFO: Uploading transactions_sample.csv to $OUTPUT_BUCKET"
gsutil cp transactions_sample.csv "$OUTPUT_BUCKET/"

INPUT_URI="${OUTPUT_BUCKET}/transactions_sample.csv"
EXPORT_URI="${OUTPUT_BUCKET}/fincrime_output/"

echo "âœ… INFO: Input URI: $INPUT_URI"
echo "âœ… INFO: Export URI: $EXPORT_URI"
echo "âœ… INFO: Staging Bucket: $STAGING_BUCKET"

# Compile pipeline locally
echo "âœ… INFO: Compiling pipeline with python3 fincrime_pipeline.py"
python3 fincrime_pipeline.py
echo "âœ… INFO: Generated Vertex AI Pipeline spec at fincrime_pipeline.yaml"

# Upload pipeline + runner script to staging bucket
echo "âœ… INFO: Uploading pipeline spec and runner script to staging bucket"
gsutil cp fincrime_pipeline.yaml "$STAGING_BUCKET/"
gsutil cp run_pipeline_auto.py "$STAGING_BUCKET/"

# Generate Custom Job YAML using containerSpec
cat > custom_job.yaml <<EOF
workerPoolSpecs:
  - machineSpec:
      machineType: n1-standard-4
    replicaCount: 1
    containerSpec:
      imageUri: asia-docker.pkg.dev/vertex-ai/training/tf-cpu.2-17.py310:latest
      command:
        - /bin/bash
        - -c
      args:
        - |
          set -e
          echo "âœ… Installing dependencies..."
          pip install --no-cache-dir google-cloud-aiplatform kfp==2.5.0 python-json-logger pandas
          echo "âœ… Downloading pipeline + runner script..."
          gsutil cp ${STAGING_BUCKET}/run_pipeline_auto.py .
          gsutil cp ${STAGING_BUCKET}/fincrime_pipeline.yaml .
          echo "âœ… Launching pipeline..."
          python3 run_pipeline_auto.py \
            --project=${PROJECT} \
            --region=${REGION} \
            --staging-bucket=${STAGING_BUCKET} \
            --gcs-input-uri=${INPUT_URI} \
            --export-uri=${EXPORT_URI} \
            --model=gemini-1.5-flash
EOF

echo "âœ… INFO: Generated Vertex AI Custom Job YAML at custom_job.yaml"

# Submit job
echo "âœ… INFO: Submitting Custom Job to Vertex AI..."
gcloud ai custom-jobs create \
  --region="$REGION" \
  --display-name="fincrime-pipeline-job" \
  --config=custom_job.yaml \
  --project="$PROJECT" \
  --service-account="$SERVICE_ACCOUNT"
